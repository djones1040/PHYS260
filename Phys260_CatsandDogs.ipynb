{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b39c8b",
   "metadata": {},
   "source": [
    "# Classifying Cats Versus Dogs with a Convolutional Neural Network\n",
    "\n",
    "We're going to use online image repositories and the `tensorflow` package to determine whether a given image is of a cat versus a dog.\n",
    "\n",
    "This is roughly modified from the first part of the tutorial here: https://www.tensorflow.org/tutorials/images/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import pathlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee94ed-ec41-41d6-a577-b572c432e347",
   "metadata": {},
   "source": [
    "## Let's grab some data and fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fa4f9-e0b6-4f40-ab16-0ed4e18c79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's grab the data -- this is the quick data set\n",
    "dataset_url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "data_dir = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=dataset_url, extract=True)\n",
    "valid_dir = pathlib.Path(data_dir.replace('.zip','')+'/validation').with_suffix('')\n",
    "data_dir = pathlib.Path(data_dir.replace('.zip','')+'/train').with_suffix('')\n",
    "\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43625dcd-6033-4c7e-a8e8-e22d4ded1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is 25000 images -- works better!\n",
    "# download link from:\n",
    "# https://www.dropbox.com/scl/fi/vr0acehjcsz0iyq2yo8gr/PetImages.zip?rlkey=brbmz8mn7g0r6l8obv3tykguh&dl=0\n",
    "# and unpack in your local directory, then uncomment the following line\n",
    "\n",
    "#data_dir = pathlib.Path('PetImages').with_suffix('')\n",
    "\n",
    "#image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "#print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e646d-e841-412d-afcb-0498863b3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's see what images we're actually looking at\n",
    "cats = list(data_dir.glob('cats/*'))\n",
    "cats_1 = PIL.Image.open(str(cats[0]))\n",
    "cats_2 = PIL.Image.open(str(cats[1]))\n",
    "    \n",
    "dogs = list(data_dir.glob('dogs/*'))\n",
    "dogs_1 = PIL.Image.open(str(dogs[0]))\n",
    "\n",
    "plt.imshow(dogs_1)\n",
    "\n",
    "cats_1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23512f-f614-4775-9625-a792d48ff823",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cats_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6b289-037c-4aa2-b178-379a90560b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "# 80% will be a \"training\" sample\n",
    "# 20% will be a \"validation\" sample, which we'll test to use if it works\n",
    "# training:\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# validation:\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# reminder of our classifications\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508e972-5191-4116-9642-892d97942b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # make sure we keep images in memory\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)    \n",
    "\n",
    "# scale from 0-1 instead of 0-255 (normal RGB colors)\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "### now the hard part\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b660944-fc69-4b70-af16-dc68ef83d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convolution layer: isolates features, many fewer parameters than working with pixels individually\n",
    "### pooling layer: reducing spatial size of convolved feature, maxes computationally easier\n",
    "###    here, we're returning max value of portion of the image returned by the kernel\n",
    "###    max value selects the \"most important\" feature of the pooled region\n",
    "### activation: 0 if negative, equal to value if positive \n",
    "###     (ignores negative-valued neurons, allows training to select useful vs non-useful features)\n",
    "### dense layer: fully-connected, all the features together for training\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a03e28-d097-49dc-9be6-0cc4f592c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now test it out on a validation image\n",
    "dog_path = str(valid_dir)+'/dogs/dog.2000.jpg'\n",
    "cat_path = str(valid_dir)+'/cats/cat.2000.jpg'\n",
    "test_image = PIL.Image.open(dog_path)\n",
    "test_image_cat = PIL.Image.open(cat_path)\n",
    "test_image_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3377303-d226-4154-a0c1-b9126617a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.utils.load_img(\n",
    "    cat_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b95142-38cc-4a6b-aabc-4cf353d29769",
   "metadata": {},
   "source": [
    "## Now, can it tell me whether my photos are of cats???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c553346-37f5-4a46-8dcd-8137049c8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "easy_cats = glob.glob('EasyCats/*.jpg')\n",
    "for e in easy_cats:\n",
    "    img = tf.keras.utils.load_img(\n",
    "        e, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a4ff3-39f5-46ac-b592-e4a1b9c7b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_cats = glob.glob('DifficultCats/*.jpg')\n",
    "for h in hard_cats:\n",
    "    img = tf.keras.utils.load_img(\n",
    "        h, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6becd4-82d9-4f92-bb80-2514a7249f4f",
   "metadata": {},
   "source": [
    "## Well shit...let's try some data augmentation to handle rotation, translation\n",
    "\n",
    "And a few more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a26a9c-6b60-4b17-ad61-bc6336e8acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cdce8-aa19-49a5-9e91-6fd8add1b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(12):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be80278-c05a-42ef-87c2-26d430818cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, name=\"outputs\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e85bca-9928-4022-ab1a-22e38b2bb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f28ae4-ec31-4f8a-beca-b213832dfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcaa60-4c24-43d1-be2b-fe1d2efb6a00",
   "metadata": {},
   "source": [
    "## Let's See if we Did Any Better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952240be-ac11-4b66-b01b-f0fe7ab6a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "easy_cats = glob.glob('EasyCats/*.jpg')\n",
    "for e in easy_cats:\n",
    "    img = tf.keras.utils.load_img(\n",
    "        e, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5610d8-6e0b-499f-8827-8358084f272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_cats = glob.glob('DifficultCats/*.jpg')\n",
    "for h in hard_cats:\n",
    "    img = tf.keras.utils.load_img(\n",
    "        h, target_size=(img_height, img_width)\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabe5a8-c68d-449d-a89f-f02643735300",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1570d-2c2f-4103-a8f8-5fa2c9b4b8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
